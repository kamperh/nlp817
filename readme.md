## Course material

| # | Note | Videos |
|---:|---|---|
| 1 | [Introduction to natural language processing](notes/01_intro_notes.pdf) | [What is natural language processing?](https://youtu.be/ZxG1YFrYuOU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (15 min) |
| 2 | [Text normalization, units and edit distance](notes/02_units_distance_notes.pdf) | [A first NLP example](https://youtu.be/k4Co_47zeO4&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)<br>[Text normalization and tokenization](https://youtu.be/Y2FBKCwww50&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)<br>[Words](https://youtu.be/o_v279Ip4GU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (12 min)<br>[Morphology](https://youtu.be/zMMrn7BZefc&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (5 min)<br>[Stems and lemmas](https://youtu.be/DWsiL01hMwk&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (3 min)<br>[Byte-pair encoding (BPE)](https://youtu.be/20xtCxAAkFw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (9 min)<br>[Edit distance](https://youtu.be/C2cRO9BqlZw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (20 min) |
| 3 | [Language modelling with N-grams](notes/03_lm_ngrams_notes.pdf) | [The language modelling problem](https://youtu.be/6TjmCP7TDOg&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (10 min) <br> [N-gram language models](https://youtu.be/SLsLEYZJ2xU&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (13 min) <br> [Start and end of sentence tokens in language models ](https://youtu.be/S1t-aac0K58&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (11 min) <br> [Why use logs in language models?](https://youtu.be/l5RgDfA2R-w&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (4 min) <br> [Evaluating language models using perplexity](https://youtu.be/72eVFb7USKs&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (9 min) <br> [Language model smoothing intuition](https://youtu.be/hU3NjSMC8uI&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (6 min) <br> [Additive smoothing in language models](https://youtu.be/zHU6IptBdJU&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (10 min) <br> [Absolute discounting in language models](https://youtu.be/g-VSL7Bu5Po&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (5 min) <br> [Language model interpolation](https://youtu.be/HHzUTUbmG4k&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (11 min) <br> [Language model backoff](https://youtu.be/DdvJ6Vd4EHg&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (4 min) <br> [Kneser-Ney smoothing](https://youtu.be/9SlJ76HtjoE&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (8 min) <br> [Are N-gram language models still used today?](https://youtu.be/YwtyFWFuVKs&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (2 min) |
| 4 | [Entropy and perplexity (advanced)](notes/04_entropy_perplexity_notes.pdf) | [What is perplexity and entropy?](https://youtu.be/iNJSUmOAPwo&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (14 min) |
| 5 | [Hidden Markov models](notes/05_hmm_notes.pdf) |  |
| 6 | [Expectation maximization (advanced)](notes/06_em_notes.pdf) |  |
| 7 | [Word embeddings](notes/07_word_embeddings_notes.pdf) |  |
| 8 | [Introduction to neural networks (recap)](notes/08_nn_notes.pdf) | [Playlist](https://www.youtube.com/playlist?list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn) <br> [Video list](https://www.kamperh.com/data414/#introduction-to-neural-networks) |
| 9 | [Recurrent neural networks](notes/09_rnn_notes.pdf) |  |
| 10 | [Encoder-decoder models and attention](notes/10_encdec_attention_notes.pdf) | [A basic encoder-decoder model for machine translation](https://youtu.be/gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)<br>[Training and loss for encoder-decoder models](https://youtu.be/aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (10 min)<br>[Encoder-decoder models in general](https://youtu.be/N8AzPeAORKM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)<br>[Greedy decoding](https://youtu.be/DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (5 min)<br>[Beam search](https://youtu.be/uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)<br>[Basic attention](https://youtu.be/BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (22 min)<br>[Attention - More general](https://youtu.be/k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)<br>[Evaluating machine translation with BLEU](https://youtu.be/evDKNiNs09o&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (23 min) |
| 11 | [Self-attention and transformers](notes/11_transformers_notes.pdf) | [Intuition behind self-attention](https://youtu.be/Y9vUgnA7gxs&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (12 min)<br> [Attention recap](https://youtu.be/HOZBI1txEIg&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (6 min)<br> [Self-attention details](https://youtu.be/3Co5iwuuMcs&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (13 min)<br> [Self-attention in matrix form](https://youtu.be/Ex4qkB1rnhk&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Positional encodings in transformers](https://youtu.be/5V9gZcAd6cE&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (19 min)<br> [The clock analogy for positional encodings](https://youtu.be/BkyEZwAf-Rw&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Multi-head attention](https://youtu.be/1JyLExxxRb8&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Masking the future in self-attention](https://youtu.be/GoGHHSQ_FTw&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Cross-attention](https://youtu.be/35jpRxa81Hc&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (7 min)<br> [Transformer](https://youtu.be/nMNW_8PYX6w&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (4 min) |



## Acknowledgements

With permission, I have used content from the NLP courses taught by
[Jan Buys](https://www.janmbuys.com/) (University of Cape Town) and
[Sharon Goldwater](https://homepages.inf.ed.ac.uk/sgwater/)
(University of Edinburgh).


## License

Herman Kamper, 2022--2023  
This work is released under a Creative Commons Attribution-ShareAlike
license ([CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/)).
