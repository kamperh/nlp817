## Course material

| # | Note | Videos |
|---:|---|---|
| 1 | [Introduction to natural language processing](notes/01_intro_notes.pdf) | [Introduction to natural language processing](https://youtu.be/ZxG1YFrYuOU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (15 min) |
| 2 | [Text normalization, units and edit distance](notes/02_units_distance_notes.pdf) | [A first NLP example](https://youtu.be/k4Co_47zeO4&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)<br>[Text normalization and tokenization](https://youtu.be/Y2FBKCwww50&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)<br>[Words](https://youtu.be/o_v279Ip4GU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (12 min)<br>[Morphology](https://youtu.be/zMMrn7BZefc&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (5 min)<br>[Stems and lemmas](https://youtu.be/DWsiL01hMwk&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (3 min)<br>[Byte-pair encoding (BPE)](https://youtu.be/20xtCxAAkFw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (9 min)<br>[Edit distance](https://youtu.be/C2cRO9BqlZw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (20 min) |
| 3 | [Language modelling with N-grams](notes/03_lm_ngrams_notes.pdf) |  |
| 4 | [Entropy and perplexity (advanced)](notes/04_entropy_perplexity_notes.pdf) |  |
| 5 | [Hidden Markov models](notes/05_hmm_notes.pdf) |  |
| 6 | [Expectation maximization (advanced)](notes/06_em_notes.pdf) |  |
| 7 | [Word embeddings](notes/07_word_embeddings_notes.pdf) |  |
| 8 | [An introduction to neural networks](notes/08_nn_notes.pdf) |  |
| 9 | [Recurrent neural networks](notes/09_rnn_notes.pdf) |  |
| 10 | [Encoder-decoder models and attention](notes/10_encdec_attention_notes.pdf) | [A basic encoder-decoder model for machine translation](https://youtu.be/gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)<br>[Training and loss for encoder-decoder models](https://youtu.be/aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (10 min)<br>[Encoder-decoder models in general](https://youtu.be/N8AzPeAORKM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)<br>[Greedy decoding](https://youtu.be/DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (5 min)<br>[Beam search](https://youtu.be/uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)<br>[Basic attention](https://youtu.be/BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (22 min)<br>[Attention - More general](https://youtu.be/k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)<br>[Evaluating machine translation with BLEU](https://youtu.be/evDKNiNs09o&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (23 min) |
| 11 | [Self-attention and transformers](notes/11_transformers_notes.pdf) |  |


## Lecture notes

01. [Introduction to natural language processing](notes/01_intro_notes.pdf)
02. [Text normalization, units and edit distance](notes/02_units_distance_notes.pdf)
03. [Language modelling with N-grams](notes/03_lm_ngrams_notes.pdf)
04. [Entropy and perplexity (advanced)](notes/04_entropy_perplexity_notes.pdf)
05. [Hidden Markov models](notes/05_hmm_notes.pdf)
06. [Expectation maximization (advanced)](notes/06_em_notes.pdf)
07. [Word embeddings](notes/07_word_embeddings_notes.pdf)
08. [An introduction to neural networks](notes/08_nn_notes.pdf)
09. [Recurrent neural networks](notes/09_rnn_notes.pdf)
10. [Encoder-decoder models and attention](notes/10_encdec_attention_notes.pdf)
11. [Self-attention and transformers](notes/11_transformers_notes.pdf)



## Videos

[1. Introduction to natural language processing](https://youtu.be/ZxG1YFrYuOU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (15 min)

[2. Text normalization, units and edit distance](https://www.youtube.com/playlist?list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d)

- [A first NLP example](https://youtu.be/k4Co_47zeO4&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)
- [Text normalization and tokenization](https://youtu.be/Y2FBKCwww50&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)
- [Words](https://youtu.be/o_v279Ip4GU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (12 min)
- [Morphology](https://youtu.be/zMMrn7BZefc&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (5 min)
- [Stems and lemmas](https://youtu.be/DWsiL01hMwk&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (3 min)
- [Byte-pair encoding (BPE)](https://youtu.be/20xtCxAAkFw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (9 min)
- [Edit distance](https://youtu.be/C2cRO9BqlZw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (20 min)

[10. Encoder-decoder models and attention](https://www.youtube.com/playlist?list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV)

- [A basic encoder-decoder model for machine translation](https://youtu.be/gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)
- [Training and loss for encoder-decoder models](https://youtu.be/aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (10 min)
- [Encoder-decoder models in general](https://youtu.be/N8AzPeAORKM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)
- [Greedy decoding](https://youtu.be/DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (5 min)
- [Beam search](https://youtu.be/uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)
- [Basic attention](https://youtu.be/BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (22 min)
- [Attention - More general](https://youtu.be/k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)
- [Evaluating machine translation with BLEU](https://youtu.be/evDKNiNs09o&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (23 min)



## Acknowledgements

With permission, I have used content from the NLP courses taught by
[Jan Buys](https://www.janmbuys.com/) (University of Cape Town) and
[Sharon Goldwater](https://homepages.inf.ed.ac.uk/sgwater/)
(University of Edinburgh).


## License

Herman Kamper, 2022--2023  
This work is released under a Creative Commons Attribution-ShareAlike
license ([CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/)).
