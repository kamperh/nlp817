## Course material

| # | Note | Videos |
|---:|---|---|
| 1 | [Introduction to natural language processing](notes/01_intro_notes.pdf) | [What is natural language processing?](https://youtu.be/ZxG1YFrYuOU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (15 min) |
| 2 | [Text normalisation, units and edit distance](notes/02_units_distance_notes.pdf) | [A first NLP example](https://youtu.be/k4Co_47zeO4&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)<br>[Text normalisation and tokenisation](https://youtu.be/Y2FBKCwww50&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (8 min)<br>[Words](https://youtu.be/o_v279Ip4GU&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (12 min)<br>[Morphology](https://youtu.be/zMMrn7BZefc&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (5 min)<br>[Stems and lemmas](https://youtu.be/DWsiL01hMwk&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (3 min)<br>[Byte-pair encoding (BPE)](https://youtu.be/20xtCxAAkFw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (9 min)<br>[Edit distance](https://youtu.be/C2cRO9BqlZw&list=PLmZlBIcArwhOqEQwyk2TBHmtEKTGPMu5d) (20 min) |
| 3 | [Language modelling with N-grams](notes/03_lm_ngrams_notes.pdf) | [The language modelling problem](https://youtu.be/6TjmCP7TDOg&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (10 min) <br> [N-gram language models](https://youtu.be/SLsLEYZJ2xU&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (13 min) <br> [Start and end of sentence tokens in language models ](https://youtu.be/S1t-aac0K58&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (11 min) <br> [Why use log in language models?](https://youtu.be/l5RgDfA2R-w&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (4 min) <br> [Evaluating language models using perplexity](https://youtu.be/72eVFb7USKs&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (9 min) <br> [Language model smoothing intuition](https://youtu.be/hU3NjSMC8uI&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (6 min) <br> [Additive smoothing in language models](https://youtu.be/zHU6IptBdJU&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (10 min) <br> [Absolute discounting in language models](https://youtu.be/g-VSL7Bu5Po&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (5 min) <br> [Language model interpolation](https://youtu.be/HHzUTUbmG4k&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (11 min) <br> [Language model backoff](https://youtu.be/DdvJ6Vd4EHg&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (4 min) <br> [Kneser-Ney smoothing](https://youtu.be/9SlJ76HtjoE&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (8 min) <br> [Are N-gram language models still used today?](https://youtu.be/YwtyFWFuVKs&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (2 min) |
| 4 | [Entropy and perplexity (advanced)](notes/04_entropy_perplexity_notes.pdf) | [What are perplexity and entropy?](https://youtu.be/iNJSUmOAPwo&list=PLmZlBIcArwhP-ril7Xe5vDNpdMEgOjppP) (14 min) |
| 5 | [Hidden Markov models](notes/05_hmm_notes.pdf) | [A first hidden Markov model example](https://youtu.be/pnGCxBjvJW0&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (14 min) <br> [Hidden Markov model definition](https://youtu.be/oGO-2dtE82Q&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (9 min) <br> [The three HMM problems](https://youtu.be/DzkKL9vyZEA&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (3 min) <br> [The Viterbi algorithm for HMMs](https://youtu.be/u4IBPD43VuY&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (24 min) <br> [Viterbi HMM example](https://youtu.be/14fC-uo7vD0&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (19 min) <br> [Why do we want the marginal probability in an HMM](https://youtu.be/84LYEt1tIzU&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH)? (7 min) <br> [The forward algorithm for HMMs](https://youtu.be/n-aVBfVNyDE&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (19 min) <br> [Learning in HMMs](https://youtu.be/Psh8fAUYrEM&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (8 min) <br> [Hard expectation maximisation for HMMs](https://youtu.be/O50_FcUEvZw&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (12 min) <br> [Soft expectation maximisation for HMMs](https://youtu.be/E6r6w44UYfo&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (20 min) <br> [Why expectation maximisation works](https://youtu.be/_3JzlDjLc6Q&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (12 min) <br> [The log-sum-exp trick](https://youtu.be/MZ2VM32h37g&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (9 min) <br> [Hidden Markov models in practice](https://youtu.be/3AGW9uj4uyE&list=PLmZlBIcArwhMIRdgNwFUWGqY53h2TC6PH) (4 min) |
| 6 | [Expectation maximisation (advanced)](notes/06_em_notes.pdf) |  |
| 7 | [Word embeddings](notes/07_word_embeddings_notes.pdf) | [Why word embeddings?](https://youtu.be/kkpRD3jS2zc&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (9 min) <br> [One-hot word embeddings](https://youtu.be/LiWmgeUZi6Y&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (6 min) <br> [Skip-gram introduction](https://youtu.be/3l2xL12CRRM&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (7 min) <br> [Skip-gram loss function](https://youtu.be/xXlzOkqQqGg&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (8 min) <br> [Skip-gram model structure](https://youtu.be/4hRGng648uQ&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (8 min) <br> [Skip-gram optimisation](https://youtu.be/Nxp7Wbrpfsc&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (10 min) <br> [Skip-gram as a neural network](https://youtu.be/ki5o0pRv1eQ&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (10 min) <br> [Skip-gram example](https://youtu.be/FGLn3gT0oYg&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (2 min) <br> [Continuous bag-of-words (CBOW)](https://youtu.be/2X9Ze-ch3Ws&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (6 min) <br> [Skip-gram with negative sampling](https://youtu.be/CjCFJAGZEio&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (16 min) <br> [GloVe word embeddings](https://youtu.be/jeW2y9CUzZo&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (12 min) <br> [Evaluating word embeddings](https://youtu.be/As5EJIONx-U&list=PLmZlBIcArwhPN5aRBaB_yTA0Yz5RQe5A_) (21 min) |
| 8 | [Introduction to neural networks](notes/08_nn_notes.pdf) | [Playlist](https://www.youtube.com/playlist?list=PLmZlBIcArwhMHnIrNu70mlvZOwe6MqWYn) <br> [Video list](https://www.kamperh.com/data414/#introduction-to-neural-networks) |
| 9 | [Recurrent neural networks](notes/09_rnn_notes.pdf) | [From feedforward to recurrent neural networks](https://youtu.be/XImxbT8U0h8?list=PLmZlBIcArwhOSBWBgRR70xip-NnbOwSji) (15 min) <br> [RNN language model loss function](https://youtu.be/kUjXExou3Uw?list=PLmZlBIcArwhOSBWBgRR70xip-NnbOwSji) (9 min) <br> [RNN definition and computational graph](https://youtu.be/Q2UpMrpKI_M?list=PLmZlBIcArwhOSBWBgRR70xip-NnbOwSji) (3 min) <br> [Backpropagation through time](https://youtu.be/d4HRuGknQjI?list=PLmZlBIcArwhOSBWBgRR70xip-NnbOwSji) (25 min) <br> [Vanishing and exploding gradients in RNNs](https://youtu.be/VqYu8INY8co?list=PLmZlBIcArwhOSBWBgRR70xip-NnbOwSji) (13 min) <br> [Solutions to exploding and vanishing gradients (in RNNs)](https://youtu.be/xgMx_YBMU8c?list=PLmZlBIcArwhOSBWBgRR70xip-NnbOwSji) (10 min) <br> [Extensions of RNNs](https://youtu.be/iGkIIIgPFHQ?list=PLmZlBIcArwhOSBWBgRR70xip-NnbOwSji) (8 min) |
| 10 | [Encoder-decoder models and attention](notes/10_encdec_attention_notes.pdf) | [A basic encoder-decoder model for machine translation](https://youtu.be/gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)<br>[Training and loss for encoder-decoder models](https://youtu.be/aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (10 min)<br>[Encoder-decoder models in general](https://youtu.be/N8AzPeAORKM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)<br>[Greedy decoding](https://youtu.be/DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (5 min)<br>[Beam search](https://youtu.be/uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (18 min)<br>[Basic attention](https://youtu.be/BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (22 min)<br>[Attention - More general](https://youtu.be/k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (13 min)<br>[Evaluating machine translation with BLEU](https://youtu.be/evDKNiNs09o&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV) (23 min) |
| 11 | [Self-attention and transformers](notes/11_transformers_notes.pdf) | [Intuition behind self-attention](https://youtu.be/Y9vUgnA7gxs&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (12 min)<br> [Attention recap](https://youtu.be/HOZBI1txEIg&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (6 min)<br> [Self-attention details](https://youtu.be/3Co5iwuuMcs&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (13 min)<br> [Self-attention in matrix form](https://youtu.be/Ex4qkB1rnhk&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Positional encodings in transformers](https://youtu.be/5V9gZcAd6cE&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (19 min)<br> [The clock analogy for positional encodings](https://youtu.be/BkyEZwAf-Rw&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Multi-head attention](https://youtu.be/1JyLExxxRb8&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Masking the future in self-attention](https://youtu.be/GoGHHSQ_FTw&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (5 min)<br> [Cross-attention](https://youtu.be/35jpRxa81Hc&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (7 min)<br> [Transformer](https://youtu.be/nMNW_8PYX6w&list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s) (4 min) |



## Acknowledgements

With permission, I have used content from the NLP courses taught by
[Jan Buys](https://www.janmbuys.com/) (University of Cape Town) and
[Sharon Goldwater](https://homepages.inf.ed.ac.uk/sgwater/)
(University of Edinburgh).


## License

Herman Kamper, 2022--2024  
This work is released under a Creative Commons Attribution-ShareAlike
license ([CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/)).
