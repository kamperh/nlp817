## Lecture notes

01. [Introduction to natural language processing](notes/01_intro_notes.pdf)
02. [Text normalization, units and edit distance](notes/02_units_distance_notes.pdf)
03. [Language modelling with N-grams](notes/03_lm_ngrams_notes.pdf)
04. [Entropy and perplexity (advanced)](notes/04_entropy_perplexity_notes.pdf)
05. [Hidden Markov models](notes/05_hmm_notes.pdf)
06. [Expectation maximization (advanced)](notes/06_em_notes.pdf)
07. [Word embeddings](notes/07_word_embeddings_notes.pdf)
08. [An introduction to neural networks](notes/08_nn_notes.pdf)
09. [Recurrent neural networks](notes/09_rnn_notes.pdf)
10. [Encoder-decoder models and attention](notes/10_encdec_attention_notes.pdf)


## Acknowledgements

With permission, I have used content from NLP courses taught by
[Jan Buys](https://www.janmbuys.com/) (University of Cape Town) and
[Sharon Goldwater](https://homepages.inf.ed.ac.uk/sgwater/)
(University of Edinburgh).


## License

Herman Kamper, 2022.  
This work is released under a Creative Commons Attribution-ShareAlike
license ([CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/)).
